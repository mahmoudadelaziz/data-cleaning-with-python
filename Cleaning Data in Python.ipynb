{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data in Python\n",
    "It's commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time analyzing it. The time spent cleaning is vital since analyzing dirty data can lead you to draw inaccurate conclusions.\n",
    "\n",
    "Data cleaning is an essential task in data science. Without properly cleaned data, the results of any data analysis or machine learning model could be inaccurate. In this course, you will learn how to identify, diagnose, and treat a variety of data cleaning problems in Python, ranging from simple to advanced. You will deal with improper data types, check that your data is in the correct range, handle missing data, perform record linkage, and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Common data problems\n",
    "\n",
    "In this chapter, I will learn how to overcome some of the most common dirty data problems: convert data types, apply range constraints to remove future data points, and remove duplicated data points to avoid double-counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the Ride Sharing CSV\n",
    "ride_sharing = pd.read_csv(\"./data/ride_sharing_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>duration</th>\n",
       "      <th>station_A_id</th>\n",
       "      <th>station_A_name</th>\n",
       "      <th>station_B_id</th>\n",
       "      <th>station_B_name</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>user_birth_year</th>\n",
       "      <th>user_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12 minutes</td>\n",
       "      <td>81</td>\n",
       "      <td>Berry St at 4th St</td>\n",
       "      <td>323</td>\n",
       "      <td>Broadway at Kearny</td>\n",
       "      <td>5480</td>\n",
       "      <td>2</td>\n",
       "      <td>1959</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24 minutes</td>\n",
       "      <td>3</td>\n",
       "      <td>Powell St BART Station (Market St at 4th St)</td>\n",
       "      <td>118</td>\n",
       "      <td>Eureka Valley Recreation Center</td>\n",
       "      <td>5193</td>\n",
       "      <td>2</td>\n",
       "      <td>1965</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8 minutes</td>\n",
       "      <td>67</td>\n",
       "      <td>San Francisco Caltrain Station 2  (Townsend St...</td>\n",
       "      <td>23</td>\n",
       "      <td>The Embarcadero at Steuart St</td>\n",
       "      <td>3652</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4 minutes</td>\n",
       "      <td>16</td>\n",
       "      <td>Steuart St at Market St</td>\n",
       "      <td>28</td>\n",
       "      <td>The Embarcadero at Bryant St</td>\n",
       "      <td>1883</td>\n",
       "      <td>1</td>\n",
       "      <td>1979</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11 minutes</td>\n",
       "      <td>22</td>\n",
       "      <td>Howard St at Beale St</td>\n",
       "      <td>350</td>\n",
       "      <td>8th St at Brannan St</td>\n",
       "      <td>4626</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    duration  station_A_id  \\\n",
       "0           0  12 minutes            81   \n",
       "1           1  24 minutes             3   \n",
       "2           2   8 minutes            67   \n",
       "3           3   4 minutes            16   \n",
       "4           4  11 minutes            22   \n",
       "\n",
       "                                      station_A_name  station_B_id  \\\n",
       "0                                 Berry St at 4th St           323   \n",
       "1       Powell St BART Station (Market St at 4th St)           118   \n",
       "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
       "3                            Steuart St at Market St            28   \n",
       "4                              Howard St at Beale St           350   \n",
       "\n",
       "                    station_B_name  bike_id  user_type  user_birth_year  \\\n",
       "0               Broadway at Kearny     5480          2             1959   \n",
       "1  Eureka Valley Recreation Center     5193          2             1965   \n",
       "2    The Embarcadero at Steuart St     3652          3             1993   \n",
       "3     The Embarcadero at Bryant St     1883          1             1979   \n",
       "4             8th St at Brannan St     4626          2             1994   \n",
       "\n",
       "  user_gender  \n",
       "0        Male  \n",
       "1        Male  \n",
       "2        Male  \n",
       "3        Male  \n",
       "4        Male  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first few rows\n",
    "ride_sharing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25760 entries, 0 to 25759\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       25760 non-null  int64 \n",
      " 1   duration         25760 non-null  object\n",
      " 2   station_A_id     25760 non-null  int64 \n",
      " 3   station_A_name   25760 non-null  object\n",
      " 4   station_B_id     25760 non-null  int64 \n",
      " 5   station_B_name   25760 non-null  object\n",
      " 6   bike_id          25760 non-null  int64 \n",
      " 7   user_type        25760 non-null  int64 \n",
      " 8   user_birth_year  25760 non-null  int64 \n",
      " 9   user_gender      25760 non-null  object\n",
      "dtypes: int64(6), object(4)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the info of this dataframe\n",
    "ride_sharing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some data type issues here. The `duration` column type is `object`, which is how `pandas` stores strings, also the `user_gender` column is stored as `object`, while it should be `category` to facilitate analysis. \n",
    "\n",
    "Also, the column `user_type` is `int`, while it should be `category`.\n",
    "The `user_type`` column contains information on whether a user is taking a free ride and takes on the following values:\n",
    "- 1 for free riders.\n",
    "- 2 for pay per ride.\n",
    "- 3 for monthly subscribers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    25760.000000\n",
      "mean         2.008385\n",
      "std          0.704541\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          3.000000\n",
      "max          3.000000\n",
      "Name: user_type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `pandas` treats the datapoints numerically, as amounts rather than categories. We can't work with that.\n",
    "\n",
    "The `user_type` column has a finite set of possible values that represent groupings of data, so it should be converted to `category`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert user_type from integer to category\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (Data Type Constraints): Summing strings and concatenating numbers\n",
    "In this exercise, you'll be converting the string column `duration` to the type `int`. Before that however, you will need to make sure to strip `\"minutes\"` from the column in order to make sure `pandas` reads it as numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>duration</th>\n",
       "      <th>station_A_id</th>\n",
       "      <th>station_A_name</th>\n",
       "      <th>station_B_id</th>\n",
       "      <th>station_B_name</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>user_birth_year</th>\n",
       "      <th>user_gender</th>\n",
       "      <th>user_type_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12 minutes</td>\n",
       "      <td>81</td>\n",
       "      <td>Berry St at 4th St</td>\n",
       "      <td>323</td>\n",
       "      <td>Broadway at Kearny</td>\n",
       "      <td>5480</td>\n",
       "      <td>2</td>\n",
       "      <td>1959</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24 minutes</td>\n",
       "      <td>3</td>\n",
       "      <td>Powell St BART Station (Market St at 4th St)</td>\n",
       "      <td>118</td>\n",
       "      <td>Eureka Valley Recreation Center</td>\n",
       "      <td>5193</td>\n",
       "      <td>2</td>\n",
       "      <td>1965</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8 minutes</td>\n",
       "      <td>67</td>\n",
       "      <td>San Francisco Caltrain Station 2  (Townsend St...</td>\n",
       "      <td>23</td>\n",
       "      <td>The Embarcadero at Steuart St</td>\n",
       "      <td>3652</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4 minutes</td>\n",
       "      <td>16</td>\n",
       "      <td>Steuart St at Market St</td>\n",
       "      <td>28</td>\n",
       "      <td>The Embarcadero at Bryant St</td>\n",
       "      <td>1883</td>\n",
       "      <td>1</td>\n",
       "      <td>1979</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11 minutes</td>\n",
       "      <td>22</td>\n",
       "      <td>Howard St at Beale St</td>\n",
       "      <td>350</td>\n",
       "      <td>8th St at Brannan St</td>\n",
       "      <td>4626</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    duration  station_A_id  \\\n",
       "0           0  12 minutes            81   \n",
       "1           1  24 minutes             3   \n",
       "2           2   8 minutes            67   \n",
       "3           3   4 minutes            16   \n",
       "4           4  11 minutes            22   \n",
       "\n",
       "                                      station_A_name  station_B_id  \\\n",
       "0                                 Berry St at 4th St           323   \n",
       "1       Powell St BART Station (Market St at 4th St)           118   \n",
       "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
       "3                            Steuart St at Market St            28   \n",
       "4                              Howard St at Beale St           350   \n",
       "\n",
       "                    station_B_name  bike_id  user_type  user_birth_year  \\\n",
       "0               Broadway at Kearny     5480          2             1959   \n",
       "1  Eureka Valley Recreation Center     5193          2             1965   \n",
       "2    The Embarcadero at Steuart St     3652          3             1993   \n",
       "3     The Embarcadero at Bryant St     1883          1             1979   \n",
       "4             8th St at Brannan St     4626          2             1994   \n",
       "\n",
       "  user_gender user_type_cat  \n",
       "0        Male             2  \n",
       "1        Male             2  \n",
       "2        Male             3  \n",
       "3        Male             1  \n",
       "4        Male             2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ride_sharing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"11 minutes\".strip(\" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         duration duration_trim  duration_time\n",
      "0      12 minutes           12              12\n",
      "1      24 minutes           24              24\n",
      "2       8 minutes            8               8\n",
      "3       4 minutes            4               4\n",
      "4      11 minutes           11              11\n",
      "...           ...           ...            ...\n",
      "25755  11 minutes           11              11\n",
      "25756  10 minutes           10              10\n",
      "25757  14 minutes           14              14\n",
      "25758  14 minutes           14              14\n",
      "25759  29 minutes           29              29\n",
      "\n",
      "[25760 rows x 3 columns]\n",
      "The average ride duration is 11.389052795031056 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip(\"minutes\")\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype(\"int\")\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(\"The average ride duration is\", ride_sharing[\"duration_time\"].mean(), \"minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (Data Range Constraints): Tire size constraints\n",
    "In this lesson, you're going to build on top of the work you've been doing with the `ride_sharing` DataFrame. You'll be working with the `tire_sizes` column which contains data on each bike's tire size.\n",
    "\n",
    "Bicycle tire sizes could be either 26″, 27″ or 29″ and are here correctly stored as a `categorical` value. In an effort to cut maintenance costs, the ride sharing provider decided to set the maximum tire size to be 27″.\n",
    "\n",
    "In this exercise, you will make sure the `tire_sizes` column has the correct range by first converting it to an `integer`, then setting and testing the new upper limit of 27″ for tire sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tire_sizes\n",
       "29    4420\n",
       "28    4298\n",
       "27    4290\n",
       "26    4275\n",
       "30    4253\n",
       "31    4224\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating the exercise dataframe by adding the 'tire_sizes' column with the specified values\n",
    "# but with some outliers above 27\n",
    "import random\n",
    "\n",
    "ride_sharing[\"tire_sizes\"] = [random.randint(26,31) for _ in range(25760)]\n",
    "ride_sharing[\"tire_sizes\"] = ride_sharing[\"tire_sizes\"].astype(\"category\")\n",
    "ride_sharing[\"tire_sizes\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     25760\n",
      "unique        2\n",
      "top          27\n",
      "freq      21485\n",
      "Name: tire_sizes, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, \"tire_sizes\"] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (Data 'Date' Range Constraints): Back to the future\n",
    "A new update to the data pipeline feeding into the `ride_sharing` DataFrame has been updated to register each ride's date. This information is stored in the `ride_date` column of the type object, which represents strings in `pandas`.\n",
    "\n",
    "A bug was discovered which was relaying rides taken today as taken next year. To fix this, you will find all instances of the `ride_date` column that occur anytime in the future, and set the maximum possible value of this column to today's date. Before doing so, you would need to convert `ride_date` to a `datetime` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating the exercise dataframe by adding the 'ride_date' column with the specified values\n",
    "# but with some outlier dates (in the future)\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Function to generate a random date within a specified range\n",
    "def random_date(start_date, end_date):\n",
    "    time_delta = end_date - start_date\n",
    "    random_days = random.randint(0, time_delta.days)\n",
    "    random_date = start_date + timedelta(days=random_days)\n",
    "    return random_date\n",
    "\n",
    "# Define the range of dates\n",
    "start_date = datetime(2018, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "\n",
    "# Create the required column to simulate the exercise\n",
    "ride_sharing[\"ride_date\"] = [random_date(start_date, end_date) for _ in range(25760)]\n",
    "# Make it of the type 'object'\n",
    "ride_sharing[\"ride_date\"] = ride_sharing[\"ride_date\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-10\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# Convert ride_date to date\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Data Uniqueness Constraints: \n",
    "**To find duplicate rows**, you can use the `pd.df.duplicated()` method.\n",
    "Specify its arguments as such:\n",
    "- `subset`: list of column names to be checked for duplication.\n",
    "- `keep`: whether to keep the first('`first`'), last('`last`'), or all('`false`') duplicate values in the result.\n",
    "**To eliminate duplicate**, you can use the `pd.df.drop_duplicates()` method with the same arguments.\n",
    "\n",
    "**Note there are two kinds of duplicates: partial and complete.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Finding duplicates\n",
    "\n",
    "A new update to the data pipeline feeding into `ride_sharing` has added the `ride_id` column, which represents a unique identifier for each ride.\n",
    "\n",
    "The update however coincided with radically shorter average ride duration times and irregular user birth dates set in the future. Most importantly, the number of rides taken has increased by 20% overnight, leading you to think there might be both complete and incomplete duplicates in the `ride_sharing` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ride_id\n",
       "23543    1\n",
       "19511    1\n",
       "9721     1\n",
       "9912     1\n",
       "3577     1\n",
       "        ..\n",
       "21506    6\n",
       "15001    6\n",
       "17078    6\n",
       "1744     6\n",
       "9181     6\n",
       "Name: count, Length: 16332, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating the exercise dataframe by adding a column 'ride_id' with random numbers that are bound\n",
    "# to have some duplicate values\n",
    "ride_sharing[\"ride_id\"] = [random.randint(1,26_000) for _ in range(25760)]\n",
    "\n",
    "# Check for duplicates\n",
    "ride_sharing[\"ride_id\"].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>user_birth_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>8</td>\n",
       "      <td>5 minutes</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>8</td>\n",
       "      <td>23 minutes</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10052</th>\n",
       "      <td>10</td>\n",
       "      <td>7 minutes</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>10</td>\n",
       "      <td>9 minutes</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25356</th>\n",
       "      <td>17</td>\n",
       "      <td>9 minutes</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23407</th>\n",
       "      <td>25993</td>\n",
       "      <td>8 minutes</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>25994</td>\n",
       "      <td>13 minutes</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>25994</td>\n",
       "      <td>7 minutes</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13037</th>\n",
       "      <td>25999</td>\n",
       "      <td>18 minutes</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14126</th>\n",
       "      <td>25999</td>\n",
       "      <td>18 minutes</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16188 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ride_id    duration  user_birth_year\n",
       "3252         8   5 minutes             1985\n",
       "5787         8  23 minutes             1983\n",
       "10052       10   7 minutes             1983\n",
       "4019        10   9 minutes             1989\n",
       "25356       17   9 minutes             1979\n",
       "...        ...         ...              ...\n",
       "23407    25993   8 minutes             1969\n",
       "1299     25994  13 minutes             1982\n",
       "161      25994   7 minutes             1990\n",
       "13037    25999  18 minutes             1981\n",
       "14126    25999  18 minutes             1993\n",
       "\n",
       "[16188 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated(subset=[\"ride_id\"], keep=False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "duplicated_rides[['ride_id','duration','user_birth_year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all our duplicate rows in this case are *incomplete duplicates.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Treating duplicates\n",
    "In the last exercise, you were able to verify that the new update feeding into `ride_sharing` contains a bug generating incomplete duplicated rows for some values of the `ride_id` column, with occasional discrepant values for the `user_birth_year` and `duration` columns.\n",
    "\n",
    "In this exercise, you will be treating those duplicated rows by first dropping complete duplicates (if any), and then merging the incomplete duplicate rows into one while keeping the average `duration`, and the minimum `user_birth_year` for each set of incomplete duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simulate the exercise dataframe\n",
    "ride_sharing[\"duration\"] = ride_sharing[\"duration\"].str.strip(\"minutes\").astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates from ride_sharing (if any)\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': \"min\", 'duration': \"mean\"}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Text and categorical data problems\n",
    "\n",
    "Categorical and text data can often be some of the messiest parts of a dataset due to their unstructured nature. In this chapter, you’ll learn how to fix whitespace and capitalization inconsistencies in category labels, collapse multiple categories into one, and reformat strings for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Membership constraints:\n",
    "We can have inconsistencies in categorical data for a variety of reasons such as data entry errors or parsing errors.\n",
    "Treatments: dropping data, remapping categories, inferring categories.\n",
    "\n",
    "It's always good practice to keep a log of all possible values of our categorical data, as it will make dealing with inconsistencies much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 4}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember from the set theory\n",
    "set_A = {1,2,3,4}\n",
    "set_B = {1,3,5,7,9}\n",
    "\n",
    "# A left anti-join\n",
    "set_A.difference(set_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Finding consistency\n",
    "In this exercise and throughout this chapter, you'll be working with the `airlines` DataFrame which contains survey responses on the San Francisco Airport from airline customers.\n",
    "\n",
    "The DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding `cleanliness`, `safety`, and `satisfaction`. Another DataFrame named `categories` was created, containing all correct possible values for the survey columns.\n",
    "\n",
    "In this exercise, you will use both of these DataFrames to find survey answers with inconsistent values, and drop them, effectively performing an outer and inner join on both these DataFrames as seen in the video exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleanliness</th>\n",
       "      <th>safety</th>\n",
       "      <th>satisfaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clean</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Very satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Average</td>\n",
       "      <td>Very safe</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Somewhat clean</td>\n",
       "      <td>Somewhat safe</td>\n",
       "      <td>Somewhat satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Somewhat dirty</td>\n",
       "      <td>Very unsafe</td>\n",
       "      <td>Somewhat unsatisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty</td>\n",
       "      <td>Somewhat unsafe</td>\n",
       "      <td>Very unsatisfied</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cleanliness           safety          satisfaction\n",
       "0           Clean          Neutral        Very satisfied\n",
       "1         Average        Very safe               Neutral\n",
       "2  Somewhat clean    Somewhat safe    Somewhat satisfied\n",
       "3  Somewhat dirty      Very unsafe  Somewhat unsatisfied\n",
       "4           Dirty  Somewhat unsafe      Very unsatisfied"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the categories dataframe to simulate the exercise\n",
    "cleanliness = ['Clean', 'Average', 'Somewhat clean', 'Somewhat dirty', 'Dirty']\n",
    "safety = ['Neutral', 'Very safe', 'Somewhat safe', 'Very unsafe', 'Somewhat unsafe']\n",
    "satisfaction = ['Very satisfied',\n",
    " 'Neutral',\n",
    " 'Somewhat satisfied',\n",
    " 'Somewhat unsatisfied',\n",
    " 'Very unsatisfied']\n",
    "\n",
    "categories = pd.DataFrame({\"cleanliness\": cleanliness, \"safety\": safety, \"satisfaction\": satisfaction})\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cleanliness           safety          satisfaction\n",
      "0           Clean          Neutral        Very satisfied\n",
      "1         Average        Very safe               Neutral\n",
      "2  Somewhat clean    Somewhat safe    Somewhat satisfied\n",
      "3  Somewhat dirty      Very unsafe  Somewhat unsatisfied\n",
      "4           Dirty  Somewhat unsafe      Very unsatisfied\n",
      "Cleanliness:  ['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty'] \n",
      "\n",
      "Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n",
      "\n",
      "Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
      " 'Very unsatisfied'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solving the exercise (Part 1 out of 3)\n",
    "# Loading the dataframe\n",
    "airlines = pd.read_csv(\"./data/airlines_final.csv\")\n",
    "\n",
    "# Print categories DataFrame\n",
    "print(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Somewhat satsified'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "set(airlines[\"satisfaction\"]).difference(categories[\"satisfaction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0    id        day         airline   destination  \\\n",
      "3              3  1157    Tuesday       SOUTHWEST   LOS ANGELES   \n",
      "4              4  2992  Wednesday        AMERICAN         MIAMI   \n",
      "6              6  2578   Saturday         JETBLUE    LONG BEACH   \n",
      "8              9   919     Friday      AIR CANADA       TORONTO   \n",
      "10            11  1129    Tuesday       SOUTHWEST     SAN DIEGO   \n",
      "...          ...   ...        ...             ...           ...   \n",
      "2468        2800  1942    Tuesday          UNITED        BOSTON   \n",
      "2469        2801  2130   Thursday  CATHAY PACIFIC     HONG KONG   \n",
      "2471        2803  2888  Wednesday          UNITED        AUSTIN   \n",
      "2472        2804  1475    Tuesday          ALASKA  NEW YORK-JFK   \n",
      "2476        2808  2162   Saturday   CHINA EASTERN       QINGDAO   \n",
      "\n",
      "        dest_region  dest_size boarding_area   dept_time  wait_min  \\\n",
      "3           West US        Hub   Gates 20-39  2018-12-31     190.0   \n",
      "4           East US        Hub   Gates 50-59  2018-12-31     559.0   \n",
      "6           West US      Small    Gates 1-12  2018-12-31      63.0   \n",
      "8     Canada/Mexico        Hub  Gates 91-102  2018-12-31      70.0   \n",
      "10          West US     Medium   Gates 20-39  2018-12-31     540.0   \n",
      "...             ...        ...           ...         ...       ...   \n",
      "2468        EAST US      Large   Gates 70-90  2018-12-31     145.0   \n",
      "2469           Asia        Hub    Gates 1-12  2018-12-31     380.0   \n",
      "2471     Midwest US     Medium   Gates 70-90  2018-12-31      60.0   \n",
      "2472        East US        Hub   Gates 50-59  2018-12-31     280.0   \n",
      "2476           Asia      Large    Gates 1-12  2018-12-31     220.0   \n",
      "\n",
      "         cleanliness           safety        satisfaction  \n",
      "3              Clean        Very safe  Somewhat satsified  \n",
      "4     Somewhat clean        Very safe  Somewhat satsified  \n",
      "6              Clean        Very safe  Somewhat satsified  \n",
      "8     Somewhat clean    Somewhat safe  Somewhat satsified  \n",
      "10             Clean        Very safe  Somewhat satsified  \n",
      "...              ...              ...                 ...  \n",
      "2468  Somewhat clean    Somewhat safe  Somewhat satsified  \n",
      "2469  Somewhat clean    Somewhat safe  Somewhat satsified  \n",
      "2471  Somewhat clean  Somewhat unsafe  Somewhat satsified  \n",
      "2472  Somewhat clean          Neutral  Somewhat satsified  \n",
      "2476           Clean        Very safe  Somewhat satsified  \n",
      "\n",
      "[1349 rows x 13 columns]\n",
      "      Unnamed: 0    id        day      airline        destination  \\\n",
      "0              0  1351    Tuesday  UNITED INTL             KANSAI   \n",
      "1              1   373     Friday       ALASKA  SAN JOSE DEL CABO   \n",
      "2              2  2820   Thursday        DELTA        LOS ANGELES   \n",
      "5              5   634   Thursday       ALASKA             NEWARK   \n",
      "7              8  2592   Saturday   AEROMEXICO        MEXICO CITY   \n",
      "...          ...   ...        ...          ...                ...   \n",
      "2467        2799  2399  Wednesday  UNITED INTL            BEIJING   \n",
      "2470        2802   394     Friday       ALASKA        LOS ANGELES   \n",
      "2473        2805  2222   Thursday    SOUTHWEST            PHOENIX   \n",
      "2474        2806  2684     Friday       UNITED            ORLANDO   \n",
      "2475        2807  2549    Tuesday      JETBLUE         LONG BEACH   \n",
      "\n",
      "        dest_region dest_size boarding_area   dept_time  wait_min  \\\n",
      "0              Asia       Hub  Gates 91-102  2018-12-31     115.0   \n",
      "1     Canada/Mexico     Small   Gates 50-59  2018-12-31     135.0   \n",
      "2           West US       Hub   Gates 40-48  2018-12-31      70.0   \n",
      "5           East US       Hub   Gates 50-59  2018-12-31     140.0   \n",
      "7     Canada/Mexico       Hub    Gates 1-12  2018-12-31     215.0   \n",
      "...             ...       ...           ...         ...       ...   \n",
      "2467           Asia       Hub  Gates 91-102  2018-12-31     195.0   \n",
      "2470        West US       Hub   Gates 50-59  2018-12-31     115.0   \n",
      "2473        West US       Hub   Gates 20-39  2018-12-31     165.0   \n",
      "2474        East US       Hub   Gates 70-90  2018-12-31      92.0   \n",
      "2475        West US     Small    Gates 1-12  2018-12-31      95.0   \n",
      "\n",
      "         cleanliness         safety    satisfaction  \n",
      "0              Clean        Neutral  Very satisfied  \n",
      "1              Clean      Very safe  Very satisfied  \n",
      "2            Average  Somewhat safe         Neutral  \n",
      "5     Somewhat clean      Very safe  Very satisfied  \n",
      "7     Somewhat clean      Very safe         Neutral  \n",
      "...              ...            ...             ...  \n",
      "2467           Clean        Neutral  Very satisfied  \n",
      "2470           Clean      Very safe  Very satisfied  \n",
      "2473           Clean      Very safe  Very satisfied  \n",
      "2474           Clean      Very safe  Very satisfied  \n",
      "2475           Clean  Somewhat safe  Very satisfied  \n",
      "\n",
      "[1128 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines[\"satisfaction\"]).difference(categories[\"satisfaction\"])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['satisfaction'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Inconsistent categories\n",
    "\n",
    "In this exercise, you will examine two categorical columns from this DataFrame, `dest_region` and `dest_size` respectively, assess how to address them and make sure that they are cleaned and ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
      " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
      " 'Australia/New Zealand' 'middle east']\n",
      "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
      " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     ']\n"
     ]
    }
   ],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can see there are some inconsistenices in the values of the `dest_region` columns due to capitalization. Inconsistencies also appear in the `dest_size` column due to leading and trailing spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
      " 'europe' 'central/south america' 'australia/new zealand']\n",
      "['Hub' 'Small' 'Medium' 'Large']\n"
     ]
    }
   ],
   "source": [
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip(\" \")\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the values in these two columns have been properly treated to be consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Remapping categories\n",
    "\n",
    "To better understand survey respondents from airlines, you want to find out if there is a relationship between certain responses and the day of the week and wait time at the gate.\n",
    "\n",
    "The `airlines` DataFrame contains the `day` and `wait_min` columns, which are categorical and numerical respectively. The `day` column contains the exact day a flight took place, and `wait_min` contains the amount of minutes it took travelers to wait at the gate. To make your analysis easier, you want to create two new categorical variables:\n",
    "- `wait_type`: `'short'` for 0-60 min, `'medium'` for 60-180 and `long` for 180+\n",
    "- `day_week`: `'weekday'` if day is in the weekday, `'weekend'` if day is in the weekend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just created two new categorical variables, that when combined with other columns, could produce really interesting analysis. Don't forget, you can always use an `assert` statement to check your changes passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Removing titles and taking names\n",
    "\n",
    "While collecting survey respondent metadata in the airlines DataFrame, the full name of respondents was saved in the `full_name` column. However upon closer inspection, you found that a lot of the different names are prefixed by honorifics such as \"Dr.\", \"Mr.\", \"Ms.\" and \"Miss\".\n",
    "\n",
    "Your ultimate objective is to create two new columns named `first_name` and `last_name`, containing the first and last names of respondents respectively. Before doing so however, you need to remove honorifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the full_name column to simulate the original exercise\n",
    "import numpy as np\n",
    "\n",
    "RAW_FULL_NAMES = ['Melodie Stuart', 'Dominic Shannon', 'Quintessa Tillman',\n",
    "       'Dr. Christine Nicholson', 'Regina Clements', 'Colleen Harding',\n",
    "       'Kaitlin Cochran', 'Molly Norton', 'Richard Lott',\n",
    "       'Matthew Nguyen', 'Dr. Laith Decker', 'Holly Austin', 'Jaden Gray',\n",
    "       'Germaine Hurley', 'Kyle Gay', 'Zachery Diaz', 'Carolyn Hartman',\n",
    "       'Miss Alana Grant', 'Idola Acosta', 'Dara English',\n",
    "       'Miss Aurora Flores', 'Henry Sloan', 'Jared Chase',\n",
    "       'Xavier Castro', 'Holmes Fowler', 'Lucy Noel', 'Kerry Tucker',\n",
    "       'Garrison Barrett', 'Stephanie Cannon', 'Dr. Charlotte Savage',\n",
    "       'Lane Clements', 'Aimee Whitfield', 'Martena Neal',\n",
    "       'Xandra Hartman', 'Meredith Gutierrez', 'Mr. Kermit Deleon',\n",
    "       'Derek Terrell', 'Shaeleigh Mccarthy', 'Burke Leon',\n",
    "       'Mr. Clinton Holmes', 'Whoopi Tillman', 'Hamilton Gardner',\n",
    "       'Graiden Bridges', 'Sheila Robinson', 'Cameron Barlow',\n",
    "       'Kasimir Irwin', 'Ms. Lilah Chen', 'Judith Price', 'Dane Barker',\n",
    "       'Micah Bullock', 'Leonard Stevens', 'Ms. Beverly Hampton',\n",
    "       'Devin Morrison', 'Mr. Jordan Cooke', 'Miss Ann Hale',\n",
    "       'Graiden Riddle', 'Julian Stanley', 'Christine Carter',\n",
    "       'Hasad Valentine', 'Bevis Mcdowell', 'Alec Davis',\n",
    "       'Dr. Daniel Hood', 'Ms. Britanney Schmidt', 'Wanda Jackson',\n",
    "       'Quyn Henderson', 'Hammett Duncan', 'Duncan Stark', 'Jin Shannon',\n",
    "       'Fulton Meadows', 'Dr. Malik Hanson', 'Laith Espinoza',\n",
    "       'Dr. Jared Holman', 'Julie Davidson', 'Dr. Jane Harrell',\n",
    "       'Aphrodite Shannon', 'Jermaine Randall', 'Hammett Talley',\n",
    "       'Sasha Riggs', 'Dr. Damian Wynn', 'Aidan Macias', 'Sawyer Hines',\n",
    "       'Mr. Hector Caldwell', 'Abra Webb', 'Stone Price',\n",
    "       'Cheyenne Stout', 'Lareina Wall', 'Dr. Ella Pena',\n",
    "       'Quintessa Sherman', 'Ishmael Duffy', 'Ms. Willa Stuart',\n",
    "       'Gareth Hunt', 'Stewart Jacobs', 'Ms. Amaya Pate',\n",
    "       'Dr. Xavier Medina', 'Mr. Marvin Mcneil', 'Imogene Harris',\n",
    "       'Abbot Hensley', 'Miss Fiona Velez', 'Rinah Stephenson',\n",
    "       'Ms. Olivia Keith', 'Vielka Rosario', 'Lani Sawyer',\n",
    "       'Clayton Sparks', 'Oprah Ingram', 'Acton Smith', 'Demetria Byrd',\n",
    "       'Patience Galloway', 'Hoyt Alvarez', 'Dara Pennington',\n",
    "       'Ebony Davidson', 'Brent Rosario', 'Melyssa Mayer', 'Regan Kelly',\n",
    "       'Leah Barlow', 'Nathan Santos', 'Uta Mckee', 'Lawrence Gallegos',\n",
    "       'Matthew Edwards', 'Xander Wilson', 'Kelly Pittman', 'Brynne Pugh',\n",
    "       'Shea Collins', 'Hu Carver', 'Stacey Coleman', 'Kaye Mcgowan',\n",
    "       'Vivien Cobb', 'Vaughan Harrison', 'Porter Hudson', 'Carl Conway',\n",
    "       'Lyle Bradshaw', 'Hashim Walter', 'Branden Larson', 'Idola Ball',\n",
    "       'Camilla White', 'Rafael Lowery', 'Victor Leon', 'Yasir Lynch',\n",
    "       'Dr. Emerson Woodard', 'Dr. Astra Mcneil', 'Dr. Shafira Marks',\n",
    "       'Mr. Dominic Smith', 'Talon Holder', 'Ivor Wise', 'Carolyn Clay',\n",
    "       'Jerome Ruiz', 'Todd Chase', 'Gray Noel', 'Ann Sanchez',\n",
    "       'Mr. Alec Heath', 'Heidi Terry', 'Alana Velasquez',\n",
    "       'Mr. Jared York', 'Abbot Lester', 'Dr. Fulton Turner',\n",
    "       'Dr. Maggie Cortez', 'Ramona Wade', 'Dr. Lynn Thomas',\n",
    "       'Aquila Graham', 'Gareth Marks', 'Dolan Wolf', 'Julie Coffey',\n",
    "       'Emerson Hatfield', 'Claire Rios', 'Christian Doyle',\n",
    "       'Haley Oliver', 'Rigel Day', 'Clare Gould', 'Ms. Keiko Mcfarland',\n",
    "       'Duncan Chandler', 'Penelope Stark', 'Kasper Shields',\n",
    "       'Dr. Rose Fleming', 'Miss Petra Mitchell', 'Ms. Regan Lynch',\n",
    "       'Keane Bennett', 'Nash Head', 'Ainsley Riley', 'Kirestin Newton',\n",
    "       'Jakeem Hall', 'Reece Mitchell', 'Wanda Walls', 'Barry Mccray',\n",
    "       'Dr. Zahir Hardin', 'Graiden Cox', 'Miss Lara Green', 'Felix Bell',\n",
    "       'Mr. Addison Day', 'Tallulah Guzman', 'Jocelyn Guzman',\n",
    "       'Ivory Miller', 'Mr. Eaton Vazquez', 'Silas Clemons',\n",
    "       'Quinn Barry', 'Orson Pratt', 'Constance Morse',\n",
    "       'Ms. Vanna Rivera', 'Miss Venus Lowe', 'Amethyst Nieves',\n",
    "       'Miss Vivian Foreman', 'Miss Wendy Griffin']\n",
    "\n",
    "names_array = np.full(2477, np.nan, dtype='object')\n",
    "names_array[:200] = RAW_FULL_NAMES\n",
    "airlines[\"full_name\"] = names_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 14)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the purposes of this exercise alone let's dump all the NAN values for names\n",
    "airlines_with_names = airlines.dropna()\n",
    "airlines_with_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\",\"\")\n",
    "\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\",\"\")\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By normalizing full names this way, you can now easily split them into first names and last names!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Keeping it descriptive\n",
    "\n",
    "To further understand travelers' experiences in the San Francisco Airport, the quality assurance department sent out a qualitative questionnaire to all travelers who gave the airport the worst score on all possible categories. The objective behind this questionnaire is to identify common patterns in what travelers are saying about the airport.\n",
    "\n",
    "Their response is stored in the `survey_response` column. Upon a closer look, you realized a few of the answers gave the shortest possible character amount without much substance. In this exercise, you will isolate the responses with a character count higher than 40, and make sure your new DataFrame contains responses with 40 characters or more using an `assert` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>day</th>\n",
       "      <th>airline</th>\n",
       "      <th>destination</th>\n",
       "      <th>dest_region</th>\n",
       "      <th>dest_size</th>\n",
       "      <th>boarding_area</th>\n",
       "      <th>dept_time</th>\n",
       "      <th>wait_min</th>\n",
       "      <th>cleanliness</th>\n",
       "      <th>safety</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>full_name</th>\n",
       "      <th>survey_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2992</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>AMERICAN</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>east us</td>\n",
       "      <td>Hub</td>\n",
       "      <td>Gates 50-59</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>559.0</td>\n",
       "      <td>Somewhat clean</td>\n",
       "      <td>Very safe</td>\n",
       "      <td>Somewhat satsified</td>\n",
       "      <td>Regina Clements</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2578</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>JETBLUE</td>\n",
       "      <td>LONG BEACH</td>\n",
       "      <td>west us</td>\n",
       "      <td>Small</td>\n",
       "      <td>Gates 1-12</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>63.0</td>\n",
       "      <td>Clean</td>\n",
       "      <td>Very safe</td>\n",
       "      <td>Somewhat satsified</td>\n",
       "      <td>Kaitlin Cochran</td>\n",
       "      <td>Very poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1129</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>SOUTHWEST</td>\n",
       "      <td>SAN DIEGO</td>\n",
       "      <td>west us</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Gates 20-39</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>540.0</td>\n",
       "      <td>Clean</td>\n",
       "      <td>Very safe</td>\n",
       "      <td>Somewhat satsified</td>\n",
       "      <td>Laith Decker</td>\n",
       "      <td>I am not a fan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0    id        day    airline destination dest_region dest_size  \\\n",
       "4            4  2992  Wednesday   AMERICAN       MIAMI     east us       Hub   \n",
       "6            6  2578   Saturday    JETBLUE  LONG BEACH     west us     Small   \n",
       "10          11  1129    Tuesday  SOUTHWEST   SAN DIEGO     west us    Medium   \n",
       "\n",
       "   boarding_area   dept_time  wait_min     cleanliness     safety  \\\n",
       "4    Gates 50-59  2018-12-31     559.0  Somewhat clean  Very safe   \n",
       "6     Gates 1-12  2018-12-31      63.0           Clean  Very safe   \n",
       "10   Gates 20-39  2018-12-31     540.0           Clean  Very safe   \n",
       "\n",
       "          satisfaction        full_name survey_response  \n",
       "4   Somewhat satsified  Regina Clements             Bad  \n",
       "6   Somewhat satsified  Kaitlin Cochran       Very poor  \n",
       "10  Somewhat satsified     Laith Decker  I am not a fan  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating the original exercise\n",
    "\n",
    "surveys_array = np.full(2477, np.nan, dtype='object')\n",
    "RAW_SURVEYS = np.array(['It was terrible', \"I didn't like the flight\", 'I hate this ',\n",
    "       'Not a fan', 'Bad', 'Horrible', 'Very poor', 'Unacceptable flight',\n",
    "       'It was awful', 'My fllight was really unpleasant',\n",
    "       'I am not a fan', 'I had a bad flight', 'It was very bad',\n",
    "       'it was horrible', 'Terrible', 'It was substandard',\n",
    "       'I did not enjoy the flight',\n",
    "       'The airport personnell forgot to alert us of delayed flights, the bathrooms could have been cleaner',\n",
    "       'The food in the airport was really really expensive - also no automatic escalators!',\n",
    "       'One of the other travelers was really loud and talkative and was making a scene and no one did anything about it',\n",
    "       \"I don't remember answering the survey with these scores, my experience was great! \",\n",
    "       'The airport personnel kept ignoring my requests for directions ',\n",
    "       'The chair I sat in was extremely uncomfortable, I still have back pain to this day! ',\n",
    "       'I wish you were more like other airports, the flights were really disorganized! ',\n",
    "       'I was really unsatisfied with the wait times before the flight. It was unacceptable.',\n",
    "       \"The flight was okay, but I didn't really like the number of times I had to stop at security\",\n",
    "       'We were really slowed down by security measures, I missed my flight because of it! ',\n",
    "       'There was a spill on the aisle next to the bathroom and it took hours to clean!',\n",
    "       'I felt very unsatisfied by how long the flight took to take off.'],\n",
    "      dtype=object)\n",
    "surveys_array[:29] = RAW_SURVEYS\n",
    "\n",
    "airlines[\"survey_response\"] = surveys_array\n",
    "airlines_with_surveys = airlines.dropna()\n",
    "# Check\n",
    "airlines_with_surveys.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17    The airport personnell forgot to alert us of d...\n",
      "18    The food in the airport was really really expe...\n",
      "19    One of the other travelers was really loud and...\n",
      "20    I don't remember answering the survey with the...\n",
      "21    The airport personnel kept ignoring my request...\n",
      "22    The chair I sat in was extremely uncomfortable...\n",
      "23    I wish you were more like other airports, the ...\n",
      "24    I was really unsatisfied with the wait times b...\n",
      "25    The flight was okay, but I didn't really like ...\n",
      "26    We were really slowed down by security measure...\n",
      "27    There was a spill on the aisle next to the bat...\n",
      "28    I felt very unsatisfied by how long the flight...\n",
      "Name: survey_response, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines[\"survey_response\"].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey[\"survey_response\"].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These types of feedbacks are essential to improving any service. Coupled with some wordcount analysis, you can find common patterns across all survey responses in no time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Advanced data problems\n",
    "\n",
    "In this chapter, you’ll dive into more advanced data cleaning problems, such as ensuring that weights are all written in kilograms instead of pounds. You’ll also gain invaluable skills that will help you verify that values have been added correctly and that missing values don’t negatively impact your analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['November 10, 2023, 09:00:00 AM', 'November 11, 2023, 09:00:00 AM',\n",
       "       'November 12, 2023, 09:00:00 AM'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Refresher on the pandas.strftime() method.\n",
    "import pandas as pd\n",
    "\n",
    "# Create a range of dates to represent a kind of 'date' column\n",
    "rng = pd.date_range(pd.Timestamp(\"2023-11-10 09:00\"), periods=3, freq='d')\n",
    "\n",
    "# Return an Index of formatted strings specified by date_format\n",
    "rng.strftime('%B %d, %Y, %r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>Age</th>\n",
       "      <th>acct_amount</th>\n",
       "      <th>inv_amount</th>\n",
       "      <th>fund_A</th>\n",
       "      <th>fund_B</th>\n",
       "      <th>fund_C</th>\n",
       "      <th>fund_D</th>\n",
       "      <th>account_opened</th>\n",
       "      <th>last_transaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>870A9281</td>\n",
       "      <td>1962-06-09</td>\n",
       "      <td>58</td>\n",
       "      <td>63523.31</td>\n",
       "      <td>51295</td>\n",
       "      <td>30105.0</td>\n",
       "      <td>4138.0</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>15632.0</td>\n",
       "      <td>02-09-18</td>\n",
       "      <td>22-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166B05B0</td>\n",
       "      <td>1962-12-16</td>\n",
       "      <td>58</td>\n",
       "      <td>38175.46</td>\n",
       "      <td>15050</td>\n",
       "      <td>4995.0</td>\n",
       "      <td>938.0</td>\n",
       "      <td>6696.0</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>28-02-19</td>\n",
       "      <td>31-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BFC13E88</td>\n",
       "      <td>1990-09-12</td>\n",
       "      <td>34</td>\n",
       "      <td>59863.77</td>\n",
       "      <td>24567</td>\n",
       "      <td>10323.0</td>\n",
       "      <td>4590.0</td>\n",
       "      <td>8469.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>25-04-18</td>\n",
       "      <td>02-04-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F2158F66</td>\n",
       "      <td>1985-11-03</td>\n",
       "      <td>35</td>\n",
       "      <td>84132.10</td>\n",
       "      <td>23712</td>\n",
       "      <td>3908.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>6482.0</td>\n",
       "      <td>12830.0</td>\n",
       "      <td>07-11-17</td>\n",
       "      <td>08-11-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7A73F334</td>\n",
       "      <td>1990-05-17</td>\n",
       "      <td>30</td>\n",
       "      <td>120512.00</td>\n",
       "      <td>93230</td>\n",
       "      <td>12158.4</td>\n",
       "      <td>51281.0</td>\n",
       "      <td>13434.0</td>\n",
       "      <td>18383.0</td>\n",
       "      <td>14-05-18</td>\n",
       "      <td>19-07-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cust_id  birth_date  Age  acct_amount  inv_amount   fund_A   fund_B  \\\n",
       "0  870A9281  1962-06-09   58     63523.31       51295  30105.0   4138.0   \n",
       "1  166B05B0  1962-12-16   58     38175.46       15050   4995.0    938.0   \n",
       "2  BFC13E88  1990-09-12   34     59863.77       24567  10323.0   4590.0   \n",
       "3  F2158F66  1985-11-03   35     84132.10       23712   3908.0    492.0   \n",
       "4  7A73F334  1990-05-17   30    120512.00       93230  12158.4  51281.0   \n",
       "\n",
       "    fund_C   fund_D account_opened last_transaction  \n",
       "0   1420.0  15632.0       02-09-18         22-02-19  \n",
       "1   6696.0   2421.0       28-02-19         31-10-18  \n",
       "2   8469.0   1185.0       25-04-18         02-04-18  \n",
       "3   6482.0  12830.0       07-11-17         08-11-18  \n",
       "4  13434.0  18383.0       14-05-18         19-07-18  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading and exploring this chapter's data\n",
    "banking = pd.read_csv(\"./data/banking_dirty.csv\", index_col=0)\n",
    "# Take a look\n",
    "banking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many rows and columns?\n",
    "banking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>Age</th>\n",
       "      <th>acct_amount</th>\n",
       "      <th>inv_amount</th>\n",
       "      <th>fund_A</th>\n",
       "      <th>fund_B</th>\n",
       "      <th>fund_C</th>\n",
       "      <th>fund_D</th>\n",
       "      <th>account_opened</th>\n",
       "      <th>last_transaction</th>\n",
       "      <th>acct_cur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>870A9281</td>\n",
       "      <td>1962-06-09</td>\n",
       "      <td>58</td>\n",
       "      <td>63523.31</td>\n",
       "      <td>51295</td>\n",
       "      <td>30105.0</td>\n",
       "      <td>4138.0</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>15632.0</td>\n",
       "      <td>02-09-18</td>\n",
       "      <td>22-02-19</td>\n",
       "      <td>euro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166B05B0</td>\n",
       "      <td>1962-12-16</td>\n",
       "      <td>58</td>\n",
       "      <td>38175.46</td>\n",
       "      <td>15050</td>\n",
       "      <td>4995.0</td>\n",
       "      <td>938.0</td>\n",
       "      <td>6696.0</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>28-02-19</td>\n",
       "      <td>31-10-18</td>\n",
       "      <td>dollar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BFC13E88</td>\n",
       "      <td>1990-09-12</td>\n",
       "      <td>34</td>\n",
       "      <td>59863.77</td>\n",
       "      <td>24567</td>\n",
       "      <td>10323.0</td>\n",
       "      <td>4590.0</td>\n",
       "      <td>8469.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>25-04-18</td>\n",
       "      <td>02-04-18</td>\n",
       "      <td>euro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F2158F66</td>\n",
       "      <td>1985-11-03</td>\n",
       "      <td>35</td>\n",
       "      <td>84132.10</td>\n",
       "      <td>23712</td>\n",
       "      <td>3908.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>6482.0</td>\n",
       "      <td>12830.0</td>\n",
       "      <td>07-11-17</td>\n",
       "      <td>08-11-18</td>\n",
       "      <td>dollar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7A73F334</td>\n",
       "      <td>1990-05-17</td>\n",
       "      <td>30</td>\n",
       "      <td>120512.00</td>\n",
       "      <td>93230</td>\n",
       "      <td>12158.4</td>\n",
       "      <td>51281.0</td>\n",
       "      <td>13434.0</td>\n",
       "      <td>18383.0</td>\n",
       "      <td>14-05-18</td>\n",
       "      <td>19-07-18</td>\n",
       "      <td>euro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cust_id  birth_date  Age  acct_amount  inv_amount   fund_A   fund_B  \\\n",
       "0  870A9281  1962-06-09   58     63523.31       51295  30105.0   4138.0   \n",
       "1  166B05B0  1962-12-16   58     38175.46       15050   4995.0    938.0   \n",
       "2  BFC13E88  1990-09-12   34     59863.77       24567  10323.0   4590.0   \n",
       "3  F2158F66  1985-11-03   35     84132.10       23712   3908.0    492.0   \n",
       "4  7A73F334  1990-05-17   30    120512.00       93230  12158.4  51281.0   \n",
       "\n",
       "    fund_C   fund_D account_opened last_transaction acct_cur  \n",
       "0   1420.0  15632.0       02-09-18         22-02-19     euro  \n",
       "1   6696.0   2421.0       28-02-19         31-10-18   dollar  \n",
       "2   8469.0   1185.0       25-04-18         02-04-18     euro  \n",
       "3   6482.0  12830.0       07-11-17         08-11-18   dollar  \n",
       "4  13434.0  18383.0       14-05-18         19-07-18     euro  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding a `acct_cur` column to make the notebook consistent with the exercises\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "banking[\"acct_cur\"] = [random.choice([\"dollar\", \"euro\"]) for _ in range(100)]\n",
    "banking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 1: Uniformity\n",
    "It's imperative to make sure all your data values are in consistent units (for instance, be careful not to mix up Fahrenheit and Celsius values for temperatures).\n",
    "\n",
    "You must also ensure that all your date data are in a uniform format. You can use the `datetime` package. The `pandas.to_datetime()` method accepts most dates formats, but can raise errors if given an unrecognizable format. You don't have to memorize any formats, they're all easily searchable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Uniform currencies\n",
    "\n",
    "In this exercise and throughout this chapter, you will be working with a retail banking dataset stored in the `banking` DataFrame. The dataset contains data on the amount of money stored in accounts (`acct_amount`), their currency (`acct_cur`), amount invested (`inv_amount`), account opening date (`account_opened`), and last transaction date (`last_transaction`) that were consolidated from American and European branches.\n",
    "\n",
    "You are tasked with understanding the average account size and how investments vary by the size of account, however in order to produce this analysis accurately, you first need to unify the currency amount into dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Uniform dates\n",
    "\n",
    "After having unified the currencies of your different account amounts, you want to add a temporal dimension to your analysis and see how customers have been investing their money given the size of their account over each year. The `account_opened` column represents when customers opened their accounts and is a good proxy for segmenting customer activity and investment over time.\n",
    "\n",
    "However, since this data was consolidated from multiple sources, you need to make sure that all dates are of the same format. You will do so by converting this column into a `datetime` object, while making sure that the format is inferred and potentially incorrect formats are set to missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37    10-08-18\n",
       "59    23-05-18\n",
       "87    15-12-18\n",
       "79    21-06-18\n",
       "13    02-07-17\n",
       "Name: account_opened, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the header of account_opend to check out what the dates look like in this column\n",
    "banking['account_opened'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2018\n",
      "1     2019\n",
      "2     2018\n",
      "3     2017\n",
      "4     2018\n",
      "      ... \n",
      "95    2018\n",
      "96    2017\n",
      "97    2017\n",
      "98    2017\n",
      "99    2017\n",
      "Name: acct_year, Length: 100, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahmo\\AppData\\Local\\Temp\\ipykernel_7464\\1380747390.py:2: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
      "C:\\Users\\mahmo\\AppData\\Local\\Temp\\ipykernel_7464\\1380747390.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n"
     ]
    }
   ],
   "source": [
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print(banking['acct_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the `acct_year` column is created, a simple `.groupby()` will show you how accounts are opened on a yearly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson: Cross field validation\n",
    "Cross field validation is the use of multiple fields in your dataset to sanity check the integrity of your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: How's our data integrity?\n",
    "\n",
    "New data has been merged into the banking DataFrame that contains details on how investments in the `inv_amount` column are allocated across four different funds A, B, C and D.\n",
    "\n",
    "Furthermore, the age and birthdays of customers are now stored in the `age` and `birth_date` columns respectively.\n",
    "\n",
    "You want to understand how customers of different age groups invest. However, you want to first make sure the data you're analyzing is correct. You will do so by cross field checking values of `inv_amount` and age against the amount invested in different funds and customers' birthdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>Age</th>\n",
       "      <th>acct_amount</th>\n",
       "      <th>inv_amount</th>\n",
       "      <th>fund_A</th>\n",
       "      <th>fund_B</th>\n",
       "      <th>fund_C</th>\n",
       "      <th>fund_D</th>\n",
       "      <th>account_opened</th>\n",
       "      <th>last_transaction</th>\n",
       "      <th>acct_cur</th>\n",
       "      <th>acct_year</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>870A9281</td>\n",
       "      <td>1962-06-09</td>\n",
       "      <td>58</td>\n",
       "      <td>69875.641</td>\n",
       "      <td>51295</td>\n",
       "      <td>30105.0</td>\n",
       "      <td>4138.0</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>15632.0</td>\n",
       "      <td>2018-02-09</td>\n",
       "      <td>22-02-19</td>\n",
       "      <td>dollar</td>\n",
       "      <td>2018</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166B05B0</td>\n",
       "      <td>1962-12-16</td>\n",
       "      <td>58</td>\n",
       "      <td>38175.460</td>\n",
       "      <td>15050</td>\n",
       "      <td>4995.0</td>\n",
       "      <td>938.0</td>\n",
       "      <td>6696.0</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>2019-02-28</td>\n",
       "      <td>31-10-18</td>\n",
       "      <td>dollar</td>\n",
       "      <td>2019</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BFC13E88</td>\n",
       "      <td>1990-09-12</td>\n",
       "      <td>34</td>\n",
       "      <td>65850.147</td>\n",
       "      <td>24567</td>\n",
       "      <td>10323.0</td>\n",
       "      <td>4590.0</td>\n",
       "      <td>8469.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>02-04-18</td>\n",
       "      <td>dollar</td>\n",
       "      <td>2018</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F2158F66</td>\n",
       "      <td>1985-11-03</td>\n",
       "      <td>35</td>\n",
       "      <td>84132.100</td>\n",
       "      <td>23712</td>\n",
       "      <td>3908.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>6482.0</td>\n",
       "      <td>12830.0</td>\n",
       "      <td>2017-07-11</td>\n",
       "      <td>08-11-18</td>\n",
       "      <td>dollar</td>\n",
       "      <td>2017</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7A73F334</td>\n",
       "      <td>1990-05-17</td>\n",
       "      <td>30</td>\n",
       "      <td>132563.200</td>\n",
       "      <td>93230</td>\n",
       "      <td>12158.4</td>\n",
       "      <td>51281.0</td>\n",
       "      <td>13434.0</td>\n",
       "      <td>18383.0</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>19-07-18</td>\n",
       "      <td>dollar</td>\n",
       "      <td>2018</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cust_id  birth_date  Age  acct_amount  inv_amount   fund_A   fund_B  \\\n",
       "0  870A9281  1962-06-09   58    69875.641       51295  30105.0   4138.0   \n",
       "1  166B05B0  1962-12-16   58    38175.460       15050   4995.0    938.0   \n",
       "2  BFC13E88  1990-09-12   34    65850.147       24567  10323.0   4590.0   \n",
       "3  F2158F66  1985-11-03   35    84132.100       23712   3908.0    492.0   \n",
       "4  7A73F334  1990-05-17   30   132563.200       93230  12158.4  51281.0   \n",
       "\n",
       "    fund_C   fund_D account_opened last_transaction acct_cur acct_year  age  \n",
       "0   1420.0  15632.0     2018-02-09         22-02-19   dollar      2018   61  \n",
       "1   6696.0   2421.0     2019-02-28         31-10-18   dollar      2019   61  \n",
       "2   8469.0   1185.0     2018-04-25         02-04-18   dollar      2018   33  \n",
       "3   6482.0  12830.0     2017-07-11         08-11-18   dollar      2017   38  \n",
       "4  13434.0  18383.0     2018-05-14         19-07-18   dollar      2018   39  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating the dataframe for the exercises\n",
    "\n",
    "banking['age'] = [61, 61, 33, 38, 39, 43, 52, 60, 48, 58, 41, 57, 39, 48, 45, 52, 45,\n",
    "       40, 58, 30, 61, 61, 31, 55, 31, 39, 31, 42, 34, 34, 56, 36, 62, 42,\n",
    "       37, 30, 39, 61, 34, 51, 50, 49, 40, 53, 31, 48, 34, 61, 60, 62, 44,\n",
    "       52, 45, 31, 35, 32, 51, 49, 53, 55, 51, 48, 32, 48, 41, 57, 40, 33,\n",
    "       56, 31, 35, 54, 51, 49, 36, 49, 47, 43, 49, 51, 53, 32, 50, 60, 50,\n",
    "       49, 49, 55, 56, 55, 39, 49, 33, 60, 62, 49, 34, 39, 54, 30]\n",
    "\n",
    "banking[\"fund_A\"] = np.array([3.01050e+04, 4.99500e+03, 1.03230e+04, 3.90800e+03, 1.21584e+04,\n",
    "       1.26860e+04, 1.79600e+03, 1.52900e+04, 1.29390e+04, 1.89660e+04,\n",
    "       2.95930e+04, 2.15600e+03, 1.83140e+04, 3.08560e+04, 1.82000e+02,\n",
    "       4.13300e+03, 8.11700e+03, 6.28600e+03, 5.50400e+03, 1.18280e+04,\n",
    "       2.93000e+03, 1.40970e+04, 8.20600e+03, 1.60920e+04, 2.80800e+03,\n",
    "       4.67600e+03, 3.88100e+03, 3.01500e+03, 1.45100e+03, 5.81790e+04,\n",
    "       2.51500e+03, 3.19300e+03, 5.38800e+04, 1.28000e+03, 2.71500e+03,\n",
    "       3.68400e+03, 9.35000e+02, 1.28000e+04, 9.85000e+02, 2.38490e+04,\n",
    "       3.80000e+02, 8.30300e+03, 5.73100e+03, 3.14000e+02, 9.10900e+03,\n",
    "       1.01070e+04, 6.08700e+03, 3.37640e+04, 9.13000e+02, 8.06300e+03,\n",
    "       2.39000e+02, 3.86410e+04, 2.18800e+03, 2.55950e+04, 6.12900e+03,\n",
    "       5.66500e+03, 2.32530e+04, 8.99200e+03, 2.37610e+04, 1.35210e+04,\n",
    "       4.10000e+01, 1.54160e+04, 6.27000e+02, 1.03730e+04, 1.17000e+02,\n",
    "       2.86150e+04, 2.48500e+03, 1.47100e+04, 4.71700e+03, 9.95000e+02,\n",
    "       2.12600e+03, 6.51100e+03, 2.62370e+04, 1.38300e+03, 9.59900e+03,\n",
    "       1.86340e+04, 4.53620e+04, 2.65410e+04, 4.59000e+03, 1.18500e+03,\n",
    "       2.60000e+02, 2.71720e+04, 1.68630e+04, 1.32650e+04, 1.34000e+03,\n",
    "       3.26920e+04, 7.93800e+03, 2.50870e+04, 2.87220e+04, 8.48936e+03,\n",
    "       4.06940e+04, 2.36000e+03, 6.46700e+03, 1.73400e+03, 1.20610e+04,\n",
    "       1.90000e+02, 2.45300e+03, 3.35200e+03, 1.75800e+03, 2.18400e+03])\n",
    "\n",
    "banking[\"fund_B\"] = np.array([ 4138.  ,   938.  ,  4590.  ,   492.  , 51281.  , 19776.  ,\n",
    "         312.  ,  3991.  ,  7757.  ,   523.  , 34379.  ,  2891.  ,\n",
    "        1477.  ,  2170.  ,  5064.  ,  8540.  ,  8777.  ,  1050.  ,\n",
    "        4063.  , 13372.  ,  7216.  , 41086.  , 15019.  ,  5491.  ,\n",
    "       22831.  ,  7349.  , 11981.  , 48358.  , 13278.  ,  6874.  ,\n",
    "        1149.  , 15847.  ,  1325.  ,  8191.  ,  2949.  , 17635.  ,\n",
    "         617.  , 30024.  ,  1008.  , 23025.  ,  2402.  , 24112.  ,\n",
    "       10608.  ,  6072.28,  6706.  ,   393.  ,  4387.  ,  5042.  ,\n",
    "       13798.  ,  1165.  ,   965.  ,  4210.  , 35508.  ,  7987.  ,\n",
    "       16840.  , 30304.  , 14273.  , 19294.  ,  4039.  , 13303.  ,\n",
    "       13870.  , 18845.  ,  1044.  ,  5583.  ,  1198.  , 21720.05,\n",
    "        1260.  , 26855.  ,  4516.  ,  3491.  , 11594.  ,  2004.  ,\n",
    "       21745.  ,  5033.  ,   858.  ,  1763.  , 17808.  , 27491.  ,\n",
    "       24786.  , 10443.  , 10152.  , 11937.  , 12793.  ,  5750.  ,\n",
    "        3676.  , 30405.  ,  3077.  , 10060.  , 29798.  , 28592.  ,\n",
    "       15792.  , 25434.  , 20861.  , 12765.  , 15742.  ,   931.  ,\n",
    "        7892.  ,  7547.  , 11174.  , 17918.  ])\n",
    "\n",
    "banking[\"fund_C\"] = np.array([1.420000e+03, 6.696000e+03, 8.469000e+03, 6.482000e+03,\n",
    "       1.343400e+04, 2.370700e+04, 2.061000e+04, 3.672800e+04,\n",
    "       1.256900e+04, 4.903000e+03, 4.268000e+03, 4.476000e+03,\n",
    "       2.904948e+04, 2.041000e+04, 3.028000e+03, 9.595000e+03,\n",
    "       1.538000e+04, 3.690000e+03, 5.602000e+03, 1.039000e+03,\n",
    "       1.035000e+03, 1.885100e+04, 5.559600e+03, 5.098000e+03,\n",
    "       9.580000e+02, 1.007500e+04, 1.375000e+04, 1.109100e+04,\n",
    "       3.315800e+04, 6.222000e+03, 1.556700e+04, 6.908000e+03,\n",
    "       2.105000e+03, 3.462000e+03, 1.072000e+03, 1.171700e+04,\n",
    "       9.606000e+03, 2.082000e+03, 6.670000e+02, 2.802000e+03,\n",
    "       1.461200e+04, 1.577600e+04, 2.650200e+04, 1.416300e+04,\n",
    "       2.343000e+03, 4.230000e+02, 8.790000e+02, 1.065900e+04,\n",
    "       7.267400e+04, 3.831900e+04, 1.189000e+03, 2.976100e+04,\n",
    "       3.310000e+02, 1.409800e+04, 1.039700e+04, 2.676700e+04,\n",
    "       6.700000e+01, 5.106000e+03, 1.561000e+04, 8.336000e+03,\n",
    "       6.011200e+04, 2.032500e+04, 4.064000e+03, 1.669000e+03,\n",
    "       4.090000e+02, 1.190600e+04, 1.464000e+03, 1.975700e+04,\n",
    "       2.328000e+03, 5.476000e+03, 1.070000e+03, 1.100700e+04,\n",
    "       1.844000e+04, 8.460000e+02, 1.083000e+03, 7.549900e+04,\n",
    "       1.271400e+04, 1.846000e+03, 3.346000e+03, 3.576200e+04,\n",
    "       2.936000e+03, 1.715000e+03, 1.261200e+04, 5.640000e+02,\n",
    "       2.640000e+02, 1.472800e+04, 8.050000e+02, 2.189300e+04,\n",
    "       6.572000e+03, 2.439000e+03, 5.342000e+03, 2.122900e+04,\n",
    "       9.861000e+03, 3.236000e+03, 3.572500e+04, 1.451000e+03,\n",
    "       3.148600e+04, 8.486000e+03, 1.165000e+04, 6.714000e+03])\n",
    "\n",
    "banking[\"fund_D\"] = np.array([15632.  ,  2421.  ,  1185.  , 12830.  , 18383.  , 11791.  ,\n",
    "       11831.  ,  5640.  , 16120.  , 19946.  ,  8322.  ,  9144.  ,\n",
    "        5539.  , 28052.  ,  6606.  , 13669.  , 12380.  ,  3401.  ,\n",
    "       10995.  ,  5562.  , 11780.  , 16406.  ,  6182.  ,  4216.  ,\n",
    "       22380.  ,  6359.  ,  5862.  , 24526.  ,  9363.  , 12939.  ,\n",
    "       12388.  ,  9550.  ,  3485.  , 23638.  ,   545.  , 33493.  ,\n",
    "        1242.  ,  7643.  ,  1556.  , 16292.  ,  3574.  ,  3286.  ,\n",
    "        5244.  ,  7908.  ,  2282.  , 10008.  , 10219.  , 19237.41,\n",
    "         496.  ,   277.  ,   821.  , 20938.  ,  8383.  , 13801.  ,\n",
    "        2392.  ,  4176.  , 11585.  , 24994.  ,  5375.  , 29678.  ,\n",
    "       15318.  , 40022.  ,  3651.  , 13768.  ,  6419.  , 10763.  ,\n",
    "        9375.  ,  2008.  , 13687.  ,  1004.  , 10968.  ,  2435.  ,\n",
    "       16940.  ,  3319.  ,  1648.  ,   718.  ,  2010.  , 26370.  ,\n",
    "       51708.  , 22965.  ,   118.  ,  4337.  , 20423.  ,  7427.  ,\n",
    "        6433.  , 13365.  ,   171.  , 19175.  , 16828.  , 30419.  ,\n",
    "         479.  ,  6864.  , 26004.16,  4317.  , 20537.  ,  4943.  ,\n",
    "        7258.  ,  8577.  ,  5080.  ,  5333.  ])\n",
    "\n",
    "# Check it out now\n",
    "banking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inconsistent investments:  8\n"
     ]
    }
   ],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis=1) == banking[\"inv_amount\"]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~ inv_equ]\n",
    "\n",
    "# Show the number of inconsistencies found\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the birth_date columns to datetime type\n",
    "banking[\"birth_date\"] = pd.to_datetime(banking[\"birth_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inconsistent ages:  4\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking[\"birth_date\"].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = banking[\"age\"] == ages_manual\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~ age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 8 and 4 rows affected by inconsistent `inv_amount` and `age` values, respectively. In this case, it's best to investigate the underlying data sources before deciding on a course of action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Record linkage\n",
    "\n",
    "Record linkage is a powerful technique used to merge multiple datasets together, used when values have typos or different spellings. In this chapter, you'll learn how to link records by calculating the similarity between strings—you’ll then use your new skills to join two restaurant review datasets into one clean master dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
